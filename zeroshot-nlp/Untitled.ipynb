{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 21.933106154203415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Developer/Projects/nlp2-transer-learning/venv/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from main import CrossLingualPipeline\n",
    "from embedding_transform import EmbeddingTransform\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from polyglot.text import Text, Word\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "clpl = CrossLingualPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(ratio=0.5):\n",
    "    with open('data/filtered_en_nl_dict.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        all_sents = f.read().splitlines()\n",
    "        n_sents = len(all_sents)\n",
    "        \n",
    "        n_train_sents = int(n_sents * ratio)\n",
    "        training_sents = all_sents[:n_train_sents]\n",
    "        eval_sents = all_sents[n_train_sents:]\n",
    "        return training_sents, eval_sents\n",
    "\n",
    "def train_transform_matrix(transform_type=\"SGD\", save_fn=\"./models/stored_embeddings-{}.pkl\", transform_subset=None, sgd_epochs=1):\n",
    "    tokens = \"[MASK]\"\n",
    "    emb_tokens = bert_model.bert.embeddings.word_embeddings(torch.tensor([bert_tokenizer.encode(tokens, add_special_tokens=True)], device='cpu'))\n",
    "    srt_tok = emb_tokens[0, 0, :]\n",
    "    msk_tok = emb_tokens[0, 1, :]\n",
    "    end_tok = emb_tokens[0, 2, :]\n",
    "    if transform_subset:\n",
    "        save_fn = save_fn.format(transform_subset)\n",
    "    else:\n",
    "        save_fn = save_fn.format(\"all\")\n",
    "\n",
    "\n",
    "    traning_sents, _ = split_dict()\n",
    "    en_words_to_embed = [words.split('\\t')[1] for words in traning_sents]\n",
    "    nl_words_to_embed = [words.split('\\t')[0] for words in traning_sents]\n",
    "\n",
    "    if transform_subset:\n",
    "        # Only use a fraction of the most common words when calculating the\n",
    "        # transformation\n",
    "\n",
    "        tokens = []\n",
    "        for line in traning_sents:\n",
    "            tokens += [token.lower() for token in line.split()]\n",
    "        token_counts = Counter(tokens)\n",
    "        filt_token_counts = Counter({k: token_counts.get(k, 0) for k in nl_words_to_embed}).most_common(int(len(nl_words_to_embed) * transform_subset))\n",
    "        most_common_subset = [w[0] for w in filt_token_counts]\n",
    "\n",
    "        most_common_ind = []\n",
    "        for word in most_common_subset:\n",
    "            most_common_ind.append(nl_words_to_embed.index(word))\n",
    "\n",
    "        new_nl_words_to_embed = [nl_words_to_embed[i] for i in most_common_ind]\n",
    "        new_en_words_to_embed = [en_words_to_embed[i] for i in most_common_ind]\n",
    "\n",
    "        nl_words_to_embed = new_nl_words_to_embed\n",
    "        en_words_to_embed = new_en_words_to_embed\n",
    "\n",
    "    english_embeddings = clpl.get_english_embeddings(en_words_to_embed)\n",
    "    dutch_embeddings = clpl.get_dutch_embeddings(nl_words_to_embed)\n",
    "\n",
    "    transform = EmbeddingTransform(transform_type, dutch_embeddings, english_embeddings, str_tok=srt_tok, end_tok=end_tok, sgd_epochs=sgd_epochs)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings():\n",
    "    \n",
    "        transform = train_transform_matrix(sgd_epochs=50)\n",
    "        english_word_embeddings = {}\n",
    "        dutch_word_embeddings = {}\n",
    "        \n",
    "        _, eval_sents = split_dict()\n",
    "        en_words_to_embed = [words.split('\\t')[1] for words in eval_sents]\n",
    "        nl_words_to_embed = [words.split('\\t')[0] for words in eval_sents]\n",
    "\n",
    "        for word in nl_words_to_embed:\n",
    "            poly_word = Word(word, language=\"nl\")\n",
    "            dutch_word_embeddings[word] = poly_word.vector\n",
    "    \n",
    "        word_indices = torch.tensor(bert_tokenizer.encode(en_words_to_embed, add_special_tokens=True)[1:-1]).unsqueeze(0)\n",
    "        all_word_embeddings = bert_model.bert.embeddings.word_embeddings\n",
    "        english_embeddings = all_word_embeddings(word_indices)\n",
    "        \n",
    "        for index, english_word in enumerate(en_words_to_embed):\n",
    "            english_word_embeddings[english_word] = english_embeddings.squeeze()[index]\n",
    "\n",
    "        correct_words = 0\n",
    "        for dutch_word in dutch_word_embeddings:\n",
    "            \n",
    "            dutch_transformed_word = dutch_word_embeddings[dutch_word] @ transform.transform.detach().numpy()\n",
    "            best_word = (None, None, 0)\n",
    "            for index, english_word in enumerate(english_word_embeddings):\n",
    "                english_word_embedding = english_word_embeddings[english_word]\n",
    "                similarity = np.dot(english_word_embedding.detach().numpy(), dutch_transformed_word)/(np.linalg.norm(english_word_embedding.detach().numpy())*np.linalg.norm(dutch_transformed_word))\n",
    "                if similarity > best_word[2]:\n",
    "                    best_word = (english_word, list(dutch_word_embeddings.keys())[index], similarity)\n",
    "            if best_word[1] == dutch_word:\n",
    "                correct_words += 1\n",
    "#             print('{} -> {} -> {}'.format(dutch_word, best_word[0], best_word[1]))\n",
    "        print('{}/{}'.format(correct_words, len(dutch_word_embeddings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dutch embeddings: 384\n",
      "epoch: 0 loss: 15.596981406211853\n",
      "epoch: 1 loss: 9.251447260379791\n",
      "epoch: 2 loss: 6.679577589035034\n",
      "epoch: 3 loss: 5.1524423360824585\n",
      "epoch: 4 loss: 4.097177952528\n",
      "epoch: 5 loss: 3.3872907757759094\n",
      "epoch: 6 loss: 2.835872545838356\n",
      "epoch: 7 loss: 2.4036504477262497\n",
      "epoch: 8 loss: 2.0670585930347443\n",
      "epoch: 9 loss: 1.7946406453847885\n",
      "epoch: 10 loss: 1.569934755563736\n",
      "epoch: 11 loss: 1.384336806833744\n",
      "epoch: 12 loss: 1.2289033457636833\n",
      "epoch: 13 loss: 1.0948253273963928\n",
      "epoch: 14 loss: 0.9826774969696999\n",
      "epoch: 15 loss: 0.8869808912277222\n",
      "epoch: 16 loss: 0.802455946803093\n",
      "epoch: 17 loss: 0.7292896956205368\n",
      "epoch: 18 loss: 0.6653983779251575\n",
      "epoch: 19 loss: 0.6087809056043625\n",
      "epoch: 20 loss: 0.5596024468541145\n",
      "epoch: 21 loss: 0.5160383246839046\n",
      "epoch: 22 loss: 0.47702304646372795\n",
      "epoch: 23 loss: 0.44134124368429184\n",
      "epoch: 24 loss: 0.409800972789526\n",
      "epoch: 25 loss: 0.38152410089969635\n",
      "epoch: 26 loss: 0.35578494891524315\n",
      "epoch: 27 loss: 0.3324795011430979\n",
      "epoch: 28 loss: 0.31079707853496075\n",
      "epoch: 29 loss: 0.2920615114271641\n",
      "epoch: 30 loss: 0.2734808325767517\n",
      "epoch: 31 loss: 0.2582377679646015\n",
      "epoch: 32 loss: 0.24180197902023792\n",
      "epoch: 33 loss: 0.22858326695859432\n",
      "epoch: 34 loss: 0.2160781268030405\n",
      "epoch: 35 loss: 0.20491322316229343\n",
      "epoch: 36 loss: 0.19295340776443481\n",
      "epoch: 37 loss: 0.1834867550060153\n",
      "epoch: 38 loss: 0.17456921003758907\n",
      "epoch: 39 loss: 0.16512458957731724\n",
      "epoch: 40 loss: 0.1575214983895421\n",
      "epoch: 41 loss: 0.1498342975974083\n",
      "epoch: 42 loss: 0.1427295794710517\n",
      "epoch: 43 loss: 0.13627595826983452\n",
      "epoch: 44 loss: 0.13004902377724648\n",
      "epoch: 45 loss: 0.12410182692110538\n",
      "epoch: 46 loss: 0.11873779352754354\n",
      "epoch: 47 loss: 0.11386162880808115\n",
      "epoch: 48 loss: 0.1094912514090538\n",
      "epoch: 49 loss: 0.10388889908790588\n",
      "2/385\n"
     ]
    }
   ],
   "source": [
    "compare_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenvadbe61290e6a49fe899ec2651f6f95c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook_hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sz3remkP9YXm",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  sys.path.append(\"/content/drive/My Drive/ir1hw3\")\n",
        "  os.chdir('/content/drive/My Drive/ir1hw3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AuBU_9Io8T80",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnr2vSxQ_5WP",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SW5I8c_TeY61",
        "colab": {}
      },
      "source": [
        "data = dataset.get_dataset().get_data_folds()[0]\n",
        "data.read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKZiOhtFdsCB"
      },
      "source": [
        "# Pointwise LTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mvv0HcHlNvqe",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module implements a LTR (PointwiseLTR) in PyTorch.\n",
        "You should fill in code into indicated sections.\n",
        "\"\"\"\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "      torch.nn.init.normal_(m.weight, mean=2, std=2)\n",
        "      m.bias.data.fill_(0)\n",
        "\n",
        "class PointwiseLTR(nn.Module):\n",
        "  \"\"\"\n",
        "  PointwiseLTR model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden, n_classes, neg_slope):\n",
        "    \"\"\"\n",
        "    Initializes PointwiseLTR object. \n",
        "    \n",
        "    Args:\n",
        "      n_inputs: number of inputs.\n",
        "      n_hidden: latent space length\n",
        "      n_classes: number of classes of the classification problem.\n",
        "                 This number is required in order to specify the\n",
        "                 output dimensions of the PointwiseLTR\n",
        "      neg_slope: negative slope parameter for LeakyReLU\n",
        "    \"\"\"\n",
        "    super(PointwiseLTR, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "                                  nn.Linear(n_inputs, n_hidden),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Linear(n_hidden, n_classes)\n",
        "    )\n",
        "    # self.network.apply(init_weights)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Performs forward pass of the input. Here an input tensor x is transformed through \n",
        "    several layer transformations.\n",
        "    \n",
        "    Args:\n",
        "      x: input to the network\n",
        "    Returns:\n",
        "      out: outputs of the network\n",
        "    \"\"\"\n",
        "    out = self.network(x)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j71YK55rxJY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class earlyStopping():\n",
        "  def __init__(self, patience=7, save_path='models/standard.mdl'):\n",
        "      \"\"\"\n",
        "          class for determing when to stop, sets early_stop to true if \n",
        "          stopping condition is met\n",
        "          patience (int): amount of rising validations in a row until stop \n",
        "      \"\"\"\n",
        "      self.patience = patience\n",
        "      self.counter = 0\n",
        "      self.best_score = None\n",
        "      self.early_stop = False\n",
        "      self.val_loss_min = np.Inf\n",
        "      self.save_path = save_path\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    score = -val_loss\n",
        "    if self.best_score is None:\n",
        "        self.best_score = score\n",
        "        return False\n",
        "    elif score < self.best_score:\n",
        "        self.counter += 1\n",
        "        print(f\"{self.counter}/{self.patience}\")\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "            torch.save(model.state_dict(), self.save_path)\n",
        "            return True\n",
        "    else:\n",
        "        self.best_score = score\n",
        "        self.counter = 0\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPAGpO4eBvhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data_fold, calc_loss=True):\n",
        "  model.eval()\n",
        "  data_loader = DataLoader(list(zip(data_fold.feature_matrix, data_fold.label_vector)), batch_size=128, shuffle=False, num_workers=0)\n",
        "  \n",
        "  all_predicted_labels = torch.zeros(len(data_fold.label_vector))\n",
        "  vector_index = 0\n",
        "  \n",
        "  for i, (features, labels) in enumerate(data_loader):\n",
        "    \n",
        "    features, labels = features.float().to(device), labels.float().to(device)\n",
        "    batch_size = labels.size(0)\n",
        "\n",
        "\n",
        "    all_predicted_labels[vector_index: vector_index + batch_size] = model.forward(features).squeeze(1)\n",
        "    vector_index = vector_index + batch_size\n",
        "\n",
        "  if calc_loss:\n",
        "    validation_loss = criterion(all_predicted_labels.view(-1), torch.tensor(data_fold.label_vector).view(-1)).item()\n",
        "  else:\n",
        "    validation_loss = 0\n",
        "\n",
        "  arr = evl.evaluate(data_fold, all_predicted_labels.detach().numpy(), ['ndcg', 'err', 'arr'], print_results=False)\n",
        "\n",
        "  return arr, validation_loss, all_predicted_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OU1auxzuRM4_",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer, criterion, epochs, early_stopping, device, eval_every=200):\n",
        "  ndcgs = []\n",
        "  validation_losses = []\n",
        "  x = []\n",
        "  for j in range(epochs):\n",
        "    total_batches = len(train_loader)\n",
        "    for i, (features, labels) in enumerate(train_loader):\n",
        "      if early_stopping.early_stop:\n",
        "        break\n",
        "      model.train()\n",
        "      features, labels = features.float().to(device), labels.float().to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      out = model.forward(features)\n",
        "\n",
        "      loss = criterion(out.view(-1), labels.view(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      if i % eval_every == 0 and not(i == 0):\n",
        "        ndcg, validation_loss, all_predicted_labels = evaluate(model, data.validation, criterion)\n",
        "        validation_losses.append(validation_loss) \n",
        "        ndcgs.append(ndcg)\n",
        "        x.append(total_batches* j + i)\n",
        "\n",
        "        print(f\"epoch {j}, iteration {i}, train_loss {loss}, validation_loss {validation_loss}, epoch_progress {i}/ {len(train_loader)}\")\n",
        "\n",
        "        early_stopping(validation_loss, model)\n",
        "\n",
        "  return ndcgs, validation_losses, x      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9nUpJA1Txff",
        "colab": {}
      },
      "source": [
        "def train_and_evaluate(lr, batch_size, hidden_layer, model_type = 'pointwise', sped_up=False):\n",
        "  if model_type == 'pointwise':\n",
        "    model = PointwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    save_path = f\"models/pointwiseLTR_lr_{lr}_batchsize_{batch_size}_hiddenlayer{hidden_layer}.mdl\"\n",
        "    \n",
        "    train_loader = DataLoader(list(zip(data.train.feature_matrix, data.train.label_vector)), batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    early_stopping = earlyStopping(patience=3, save_path=save_path)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    ndcg, validation_loss, x = train(model, train_loader, optimizer, criterion, 10, early_stopping, device, eval_every=math.floor(len(train_loader)/3))\n",
        "\n",
        "  if model_type == 'pairwise':\n",
        "    model = PairwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "    criterion = pairwise_ltr_loss\n",
        "    save_path = f\"models/PairwiseLTR_lr_{lr}_hiddenlayer{hidden_layer}.mdl\"\n",
        "    early_stopping = earlyStopping(patience=3, save_path=save_path)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    ndcg, validation_loss, x = train(model, data.train, 10, optimizer, pairwise_ltr_loss, device, early_stopping, sped_up=sped_up, eval_every = math.floor(data.train.num_queries()/5) )\n",
        "  return (min(validation_loss).item(), max(ndcg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti4wPo2MDlzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## grid search\n",
        "# results = {}\n",
        "# for lr in [0.1,0.01, 0.001, 0.0001,0.00001, 0.000001]:\n",
        "#   batch_size = 64\n",
        "#   hidden_layer = 250\n",
        "#   results[str(lr)] = train_and_evaluate(lr, batch_size, hidden_layer)\n",
        "# print(results)\n",
        "\n",
        "# results = {}\n",
        "# for batch_size in [32,64,128,254, 512, 1024]:\n",
        "#   lr = 0.0001\n",
        "#   hidden_layer = 250\n",
        "#   results[str(batch_size)] = train_and_evaluate(lr, batch_size, hidden_layer)\n",
        "# print(results)\n",
        "\n",
        "# results = {}\n",
        "# for hidden_layer in [10, 50, 100, 250, 500, 750, 1000]:\n",
        "#   batch_size = 64\n",
        "#   lr = 0.0001\n",
        "#   results[str(hidden_layer)] = train_and_evaluate(lr, batch_size, hidden_layer)\n",
        "# print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGHr3e11iqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "hidden_layer = 750\n",
        "\n",
        "PLTR = PointwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "train_loader = DataLoader(list(zip(data.train.feature_matrix, data.train.label_vector)), batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "save_path = f\"models/pointwiseLTR_lr_{lr}_batchsize_{batch_size}_hiddenlayer{hidden_layer}.mdl\"\n",
        "\n",
        "early_stopping = earlyStopping(patience=3, save_path=save_path)\n",
        "\n",
        "optimizer = optim.Adam(PLTR.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "ndcg, validation_loss, x = train(PLTR, train_loader, optimizer, criterion, 10, early_stopping, device, eval_every=len(train_loader)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO4E0Cv3oxon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_distribution(prediction, golden, label):\n",
        "  width = 0.30\n",
        "  fig, ax = plt.subplots(figsize=(8, 4))\n",
        "  labels = [0, 1, 2, 3, 4]\n",
        "\n",
        "  ax.bar(np.array(list(prediction.keys())) - 0.5*width, prediction.values(), width, label='predictions')\n",
        "  ax.bar(np.array(list(golden.keys())) + 0.5*width, golden.values(), width, label='truth')\n",
        "\n",
        "  ax.set_ylabel('document count')\n",
        "  ax.set_title('score distribution predictions ' + label)\n",
        "  ax.set_xticks(labels)\n",
        "  ax.set_xticklabels(labels)\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def show_predictions(model, datafold, label):\n",
        "  ndcg, validation_loss, all_predicted_labels = evaluate(model, datafold)\n",
        "\n",
        "  prediction = Counter(all_predicted_labels.round().int().detach().numpy())\n",
        "  golden = Counter(datafold.label_vector)\n",
        "\n",
        "  plot_distribution(prediction, golden, label)\n",
        "\n",
        "# show_predictions(PLTR, data.validation, 'validation')\n",
        "# show_predictions(PLTR, data.test, 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sArfkrTXvBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_ndcg_validation(ndcg, validation_loss, x, loss_kind='validation loss'):\n",
        "  plt.plot(x, ndcg, label='ndcg')\n",
        "  plt.plot(x, validation_loss, label=loss_kind)\n",
        "  plt.xlabel('iterations')\n",
        "  plt.ylabel('score/loss')\n",
        "  plt.title(\"pointwise LTR validation loss and ndcg\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "# plot_ndcg_validation(ndcg, validation_loss, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YBlyxOrZeIU_"
      },
      "source": [
        "# Pairwise LTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4x81Tv0ceK79",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module implements a LTR (PairwiseLTR) in PyTorch.\n",
        "You should fill in code into indicated sections.\n",
        "\"\"\"\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "      # m.bias.data.fill_(0)\n",
        "\n",
        "class PairwiseLTR(nn.Module):\n",
        "  \"\"\"\n",
        "  PairwiseLTR model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden, n_classes, neg_slope):\n",
        "    \"\"\"\n",
        "    Initializes PointwiseLTR object. \n",
        "    \n",
        "    Args:\n",
        "      n_inputs: number of inputs.\n",
        "      n_hidden: latent space length\n",
        "      n_classes: number of classes of the classification problem.\n",
        "                 This number is required in order to specify the\n",
        "                 output dimensions of the PointwiseLTR\n",
        "      neg_slope: negative slope parameter for LeakyReLU\n",
        "    \"\"\"\n",
        "    super(PairwiseLTR, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "                                  nn.Linear(n_inputs, n_hidden),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Linear(n_hidden, n_classes)\n",
        "    )\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Performs forward pass of the input. Here an input tensor x is transformed through \n",
        "    several layer transformations.\n",
        "    \n",
        "    Args:\n",
        "      x: input to the network\n",
        "    Returns:\n",
        "      out: outputs of the network\n",
        "    \"\"\"\n",
        "    out = self.network(x)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSLRxFMUvs5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_ltr_loss_sped_up(predicted_labels, true_labels):\n",
        "  # true_labels = true_labels / 4\n",
        "  predicted_labels = predicted_labels.squeeze()\n",
        "\n",
        "  # pairs = get_pairs(predicted_labels.shape[0])\n",
        "\n",
        "  tups = list(zip(*itertools.combinations(range(true_labels.size(0)), 2))) \n",
        "  #\n",
        "  first_int, second_int = list(tups[0]), list(tups[1])\n",
        "\n",
        "  predicted_first = predicted_labels[first_int]\n",
        "  predicted_second = predicted_labels[second_int]\n",
        "  \n",
        "  true_first = true_labels[first_int]\n",
        "  true_second = true_labels[second_int]\n",
        "\n",
        "  first_larger = (true_first > true_second).type(torch.ByteTensor)\n",
        "  second_larger = (true_first < true_second).type(torch.ByteTensor)\n",
        "  S = (torch.zeros(first_larger.shape) + first_larger - second_larger).to(device)\n",
        "\n",
        "  sig = torch.sigmoid(predicted_first.float() - predicted_second.float()).to(device)\n",
        "  C_T = (0.5*(1 - S)*sig + torch.log(1 + torch.exp(-sig)))\n",
        "  return C_T.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47RIMX4uwhfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_ltr_loss(model, features, labels):\n",
        "  tups = list(zip(*itertools.combinations(range(labels.size(0)), 2))) \n",
        "  # combinations = [(1,1)(1,2)] zip(*combinations) = [[1,1][1,2]] ~ 50x faster than forloop\n",
        "  index_document_pair1, index_document_pair2 = list(tups[0]), list(tups[1])\n",
        "\n",
        "  features_first = features[index_document_pair1]\n",
        "  features_second = features[index_document_pair2]\n",
        "\n",
        "  true_first = labels[index_document_pair1]\n",
        "  true_second = labels[index_document_pair2]\n",
        "\n",
        "  predicted_first = model.forward(features_first).squeeze()\n",
        "  predicted_second = model.forward(features_second).squeeze()\n",
        "  \n",
        "  first_larger = (true_first > true_second).type(torch.ByteTensor)\n",
        "  second_larger = (true_first < true_second).type(torch.ByteTensor)\n",
        "  \n",
        "  S = (torch.zeros(first_larger.shape) + first_larger - second_larger).to(device)\n",
        "\n",
        "  sig = (predicted_first.float() - predicted_second.float()).sigmoid()\n",
        "\n",
        "  C_T = (0.5*(1 - S)*sig + torch.log(1 + torch.exp(-sig)))\n",
        "  return C_T.mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3YKLDVihopE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data_fold):\n",
        "  model.eval()\n",
        "  data_loader = DataLoader(list(zip(data_fold.feature_matrix, data_fold.label_vector)), batch_size=128, shuffle=False, num_workers=0)\n",
        "  \n",
        "  all_predicted_labels = torch.zeros(len(data_fold.label_vector))\n",
        "  vector_index = 0\n",
        "  \n",
        "  for i, (features, labels) in enumerate(data_loader):\n",
        "    features, labels = features.float().to(device), labels.float().to(device)\n",
        "    batch_size = labels.size(0)\n",
        "\n",
        "    all_predicted_labels[vector_index: vector_index + batch_size] = model.forward(features).squeeze(1)\n",
        "    vector_index = vector_index + batch_size\n",
        "\n",
        "  evaluations = evl.evaluate(data_fold, all_predicted_labels.detach().numpy(), ['ndcg', 'err', 'arr'], print_results=False)\n",
        "  err = evaluations['err'][0]\n",
        "  arr = evaluations['arr'][0]\n",
        "  ndcg = evaluations['ndcg'][0]\n",
        "  return ndcg, arr, err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEQgofnKlT3D",
        "colab": {}
      },
      "source": [
        "def train(model, train_data, epochs, optimizer, criterion, device, early_stopping, sped_up=False, eval_every = 200):\n",
        "  ndcgs = []\n",
        "  arrs = []\n",
        "  errs = []\n",
        "  x = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch))\n",
        "\n",
        "    num_queries = train_data.num_queries()\n",
        "\n",
        "    for qid in range(0, num_queries):\n",
        "      if early_stopping.early_stop:\n",
        "        break\n",
        "      \n",
        "      # get documents from one specific query\n",
        "      s_i, e_i = train_data.query_range(qid)\n",
        "      \n",
        "      if (e_i - s_i) < 2: \n",
        "        continue\n",
        "      \n",
        "      features, labels = torch.tensor(train_data.feature_matrix[s_i:e_i]).float().to(device), torch.tensor(train_data.label_vector[s_i:e_i]).float().to(device)\n",
        "      \n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if sped_up == False:\n",
        "        loss = pairwise_ltr_loss(model, features, labels)\n",
        "      else:\n",
        "        loss = pairwise_ltr_loss_sped_up(model.forward(features), labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if qid % eval_every == 0 and not(qid == 0):\n",
        "\n",
        "        ndcg, arr, err = evaluate(model, data.validation)\n",
        "        ndcgs.append(ndcg)\n",
        "        arrs.append(arr)\n",
        "        errs.append(err)\n",
        "        x.append(num_queries* epoch + qid)\n",
        "\n",
        "        print(f\"epoch {epoch}, iteration {qid}, train_loss {loss}, validation_ndcg {ndcg}, epoch_progress {qid}/ {num_queries}\")\n",
        "\n",
        "        early_stopping(-ndcg, model)\n",
        "\n",
        "  return ndcgs, arrs, errs, x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5SPpMkLcf8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import importlib\n",
        "importlib.reload(evl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msSJSXch_jzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0001\n",
        "hidden_layer = 1000\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "PairLTR = PairwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "\n",
        "optimizer = optim.Adam(PairLTR.parameters(), lr=lr)\n",
        "\n",
        "save_path = f\"models/PairwiseLTR_lr_{lr}_hiddenlayer{hidden_layer}.mdl\"\n",
        "early_stopping = earlyStopping(patience=25, save_path=save_path)\n",
        "\n",
        "ndcgs, arrs, errs, x = train(PairLTR, data.train, 10, optimizer, pairwise_ltr_loss, device, early_stopping, eval_every = 200)\n",
        "\n",
        "\n",
        "\n",
        "lr = 0.0001\n",
        "hidden_layer = 1000\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "PairLTR = PairwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "\n",
        "optimizer = optim.Adam(PairLTR.parameters(), lr=lr)\n",
        "\n",
        "save_path = f\"models/PairwiseLTR_lr_{lr}_hiddenlayer{hidden_layer}.mdl\"\n",
        "early_stopping = earlyStopping(patience=25, save_path=save_path)\n",
        "\n",
        "ndcgs_speed, arrs, errs, x_speed = train(PairLTR, data.train, 10, optimizer, pairwise_ltr_loss, device, early_stopping,sped_up=True, eval_every = 200)\n",
        "\n",
        "# plot_ndcg_validation(ndcg, validation_loss, x, loss_kind='train_loss')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCE6I7tloy2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw5QWFVJi085",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ndcgs, arrs, errs, x\n",
        "\n",
        "plt.plot(x, ndcgs, label='ndcg')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('score/loss')\n",
        "plt.title(\"pairwise LTR ndcg\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x, arrs, label='arr')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('score/loss')\n",
        "plt.title(\"pairwise LTR arr\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plt.plot(x, errs, label='err')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDLALO4vo1sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0001\n",
        "hidden_layer = 1000\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "PairLTR = PairwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "\n",
        "optimizer = optim.Adam(PairLTR.parameters(), lr=lr)\n",
        "\n",
        "save_path = f\"models/PairwiseLTR_lr_{lr}_hiddenlayer{hidden_layer}.mdl\"\n",
        "early_stopping = earlyStopping(patience=25, save_path=save_path)\n",
        "\n",
        "ndcgs, arrs, errs, x = train(PairLTR, data.train, 10, optimizer, pairwise_ltr_loss, device, early_stopping, eval_every = 200)\n",
        "\n",
        "\n",
        "lr = 0.0001\n",
        "hidden_layer = 1000\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "PairLTR = PairwiseLTR(n_inputs=data.num_features, n_hidden=hidden_layer, n_classes=1, neg_slope=0.1).to(device)\n",
        "\n",
        "optimizer = optim.Adam(PairLTR.parameters(), lr=lr)\n",
        "\n",
        "save_path = f\"models/PairwiseLTR_lr_{lr}_hiddenlayer{hidden_layer}.mdl\"\n",
        "early_stopping = earlyStopping(patience=25, save_path=save_path)\n",
        "\n",
        "ndcgs_speed, arrs, errs, x_speed = train(PairLTR, data.train, 10, optimizer, pairwise_ltr_loss, device, early_stopping,sped_up=True, eval_every = 200)\n",
        "\n",
        "# plot_ndcg_validation(ndcg, validation_loss, x, loss_kind='train_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTSTKLGJo7p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(x, ndcgs, label='ranknet')\n",
        "plt.plot(x_speed, ndcgs_speed, label='ranknet: speedup')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('score')\n",
        "plt.title(\"ranknet convergence vs speedup\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eqrXHhOYrsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  evaluate(PairLTR, data.test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utSmizVEFIkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# results = {}\n",
        "# for lr in [0.1,0.01, 0.001, 0.0001,0.00001, 0.000001]:\n",
        "#   batch_size = 0\n",
        "#   hidden_layer = 250\n",
        "#   results[str(lr)] = train_and_evaluate(lr, batch_size, hidden_layer, sped_up=True, model_type = 'pairwise')\n",
        "# print(results)\n",
        "\n",
        "# results = {}\n",
        "# for hidden_layer in [50, 100, 250, 500, 750, 1000, 1500]:\n",
        "#   batch_size = 0\n",
        "#   lr = 0.0001\n",
        "#   results[str(hidden_layer)] = train_and_evaluate(lr, batch_size, hidden_layer,sped_up=True, model_type = 'pairwise')\n",
        "# print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91xQJLd18T9J"
      },
      "source": [
        "NDCG\n",
        "Deep learning for model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HwzIKJ7t8T9L"
      },
      "source": [
        "AQ2.2 (10 points) Compute a distribution of the scores (if you’re using a classification\n",
        "loss, use the argmax) output by your model on the validation and test sets. Compare this\n",
        "with the distribution of the actual grades. If your distributions don’t match, reflect on\n",
        "how you can fix this and if your solution is sufficient for LTR.\n",
        "\n",
        "\n",
        "210.000.000 vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lkzvMGxDdU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjgAAzn0WX5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}